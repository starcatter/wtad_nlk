{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('why arent you going because i dont want to', '[start] dlaczego nie idziesz bo nie chce [end]')\n",
      "('i dont have an answer', '[start] nie posiadam odpowiedzi [end]')\n",
      "('please remember to mail the letters', '[start] pamietaj aby wyslac te listy [end]')\n",
      "('come here', '[start] chodz tu [end]')\n",
      "('im afraid i cant afford to buy a new car', '[start] obawiam sie ze nie stac mnie na zakup nowego samochodu [end]')\n",
      "41666 total pairs\n",
      "29168 training pairs\n",
      "6249 validation pairs\n",
      "6249 test pairs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 15:57:05.872978: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:05.919044: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:05.919235: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:05.920010: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-06-22 15:57:05.948294: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:05.948516: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:05.948665: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:06.500409: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:06.500584: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:06.500724: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-06-22 15:57:06.500852: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5752 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2070 SUPER, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "# assemble text data for english->polish translation\n",
    "translator.prepare_text_zip(\"in/pol-eng.zip\",\"pol.txt\",\"lang/eng-pol\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above will clean the text input, split the data into training/verification/test sets, and setup and fit vectorizers for training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('do ktorego klubu nalezysz', '[start] what club do you belong to [end]')\n",
      "('juz prawie czas spac', '[start] its almost bedtime [end]')\n",
      "('niebo jest zachmurzone', '[start] its cloudy [end]')\n",
      "('tom prawie powiedzial mary ze ja kocha', '[start] tom almost told mary that he loved her [end]')\n",
      "('nie umiem obslugiwac tej maszyny', '[start] i cant figure out how to operate this machine [end]')\n",
      "41666 total pairs\n",
      "29168 training pairs\n",
      "6249 validation pairs\n",
      "6249 test pairs\n"
     ]
    }
   ],
   "source": [
    "# text data for reverse translation\n",
    "translator.prepare_text_zip(\"in/pol-eng.zip\",\"pol.txt\",\"lang/pol-eng\", flip_translation=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (256, 16)\n",
      "inputs[\"decoder_inputs\"].shape: (256, 16)\n",
      "targets.shape: (256, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 15:57:11.530380: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   12804096    ['encoder_inputs[0][0]']         \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " TransformerEncoder (Transforme  (None, None, 256)   785652      ['token_and_position_embedding[0]\n",
      " rEncoder)                                                       [0]']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 50000)  29336656    ['decoder_inputs[0][0]',         \n",
      "                                                                  'TransformerEncoder[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,926,404\n",
      "Trainable params: 42,926,404\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 15:57:14.885246: I tensorflow/stream_executor/cuda/cuda_dnn.cc:384] Loaded cuDNN version 8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114/114 [==============================] - 28s 224ms/step - loss: 3.0459 - accuracy: 0.6755 - val_loss: 2.3498 - val_accuracy: 0.7004\n",
      "Epoch 2/4\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 2.2041 - accuracy: 0.7100 - val_loss: 2.1388 - val_accuracy: 0.7258\n",
      "Epoch 3/4\n",
      "114/114 [==============================] - 23s 201ms/step - loss: 1.9158 - accuracy: 0.7407 - val_loss: 1.9270 - val_accuracy: 0.7490\n",
      "Epoch 4/4\n",
      "114/114 [==============================] - 21s 188ms/step - loss: 1.7017 - accuracy: 0.7598 - val_loss: 1.7794 - val_accuracy: 0.7658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, position_embedding_layer_call_fn, position_embedding_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 916 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd88416b7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 916 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd88416b7f0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/4\n",
      "114/114 [==============================] - 23s 190ms/step - loss: 1.7061 - accuracy: 0.7656 - val_loss: 1.6889 - val_accuracy: 0.7727\n",
      "Epoch 2/4\n",
      "114/114 [==============================] - 22s 190ms/step - loss: 1.4573 - accuracy: 0.7837 - val_loss: 1.6708 - val_accuracy: 0.7791\n",
      "Epoch 3/4\n",
      "114/114 [==============================] - 22s 191ms/step - loss: 1.3228 - accuracy: 0.7942 - val_loss: 1.5726 - val_accuracy: 0.7793\n",
      "Epoch 4/4\n",
      "114/114 [==============================] - 23s 201ms/step - loss: 1.2035 - accuracy: 0.8043 - val_loss: 1.5336 - val_accuracy: 0.7904\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, restored_function_body, restored_function_body, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    }
   ],
   "source": [
    "# create and pre-train translation model\n",
    "translator.create_translator(\"lang/eng-pol\", \"trans/eng-pol\")\n",
    "\n",
    "# train the model some more\n",
    "translator.train_model(\"lang/eng-pol\", \"trans/eng-pol\", 4)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 917 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd8e8661a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 917 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7fd8e8661a20> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[Hello world] -> out:[ uspokoj ]\n",
      "in:[Good morning] -> out:[ trzymaj rano ]\n"
     ]
    }
   ],
   "source": [
    "# translate some phrases\n",
    "translator.translate_sequences(\"lang/eng-pol\", \"trans/eng-pol\", [\"Hello world\", \"Good morning\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[theyre just lazy] -> out:[ oni tylko tylko ]\n",
      "in:[she wants me to go with her] -> out:[ ona ze mna do niej ]\n",
      "in:[clearly you are mistaken] -> out:[ to prawda ]\n",
      "in:[make another appointment at the front desk] -> out:[ po sie na pol ]\n",
      "in:[he still has a white vest] -> out:[ on ma ma ]\n",
      "in:[we have a problem tom] -> out:[ mamy problem ]\n",
      "in:[he made it for his sister] -> out:[ on to dla ]\n",
      "in:[i am from shizuoka] -> out:[ jestem z powodu ]\n",
      "in:[my thirteen year old girl loves to watch romance movies] -> out:[ moja lata roku sie z tych przez ]\n",
      "in:[ill join you in a moment] -> out:[ bede do ciebie chwile ]\n"
     ]
    }
   ],
   "source": [
    "# translate some sequences from file\n",
    "with open(\"lang/eng-pol/test_texts.txt\", \"rt\") as file_in:\n",
    "\tlines = file_in.readlines()\n",
    "\ttranslator.translate_sequences(\"lang/eng-pol\", \"trans/eng-pol\", random.sample(lines, 10))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (256, 16)\n",
      "inputs[\"decoder_inputs\"].shape: (256, 16)\n",
      "targets.shape: (256, 16)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-22 16:00:28.383132: W tensorflow/core/kernels/data/cache_dataset_ops.cc:856] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_2  (None, None, 256)   12804096    ['encoder_inputs[0][0]']         \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " TransformerEncoder (Transforme  (None, None, 256)   785652      ['token_and_position_embedding_2[\n",
      " rEncoder)                                                       0][0]']                          \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, None, 50000)  29336656    ['decoder_inputs[0][0]',         \n",
      "                                                                  'TransformerEncoder[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,926,404\n",
      "Trainable params: 42,926,404\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/8\n",
      "114/114 [==============================] - 24s 194ms/step - loss: 3.0354 - accuracy: 0.6195 - val_loss: 2.3283 - val_accuracy: 0.6429\n",
      "Epoch 2/8\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 2.1735 - accuracy: 0.6552 - val_loss: 2.0977 - val_accuracy: 0.6751\n",
      "Epoch 3/8\n",
      "114/114 [==============================] - 25s 222ms/step - loss: 1.8867 - accuracy: 0.6919 - val_loss: 1.8349 - val_accuracy: 0.7099\n",
      "Epoch 4/8\n",
      "114/114 [==============================] - 24s 213ms/step - loss: 1.6350 - accuracy: 0.7237 - val_loss: 1.7062 - val_accuracy: 0.7259\n",
      "Epoch 5/8\n",
      "114/114 [==============================] - 24s 213ms/step - loss: 1.4218 - accuracy: 0.7497 - val_loss: 1.6066 - val_accuracy: 0.7329\n",
      "Epoch 6/8\n",
      "114/114 [==============================] - 24s 212ms/step - loss: 1.2490 - accuracy: 0.7702 - val_loss: 1.4438 - val_accuracy: 0.7594\n",
      "Epoch 7/8\n",
      "114/114 [==============================] - 24s 211ms/step - loss: 1.1012 - accuracy: 0.7885 - val_loss: 1.4333 - val_accuracy: 0.7595\n",
      "Epoch 8/8\n",
      "114/114 [==============================] - 25s 217ms/step - loss: 0.9790 - accuracy: 0.8042 - val_loss: 1.3810 - val_accuracy: 0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, position_embedding_2_layer_call_fn, position_embedding_2_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/pol-eng/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/pol-eng/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[nie musisz krzyczec] -> out:[ you dont need ]\n",
      "in:[nikt nie wie jak sie czuje] -> out:[ nobody one knows how how ]\n",
      "in:[zgubilismy sie w lesie] -> out:[ we lost in the woods ]\n",
      "in:[sadze ze bylam zbyt zajeta by zauwazyc ze tom mial problemy] -> out:[ i think i was have tom would that he had a new ]\n",
      "in:[tom wszedl niosac szesciopak piwa] -> out:[ tom walked the bell of ]\n",
      "in:[tom nigdy nie bedzie cie kochal] -> out:[ tom would never to hurt you ]\n",
      "in:[jestesmy kreatywni] -> out:[ were are ]\n",
      "in:[nie chce cie] -> out:[ i dont want to you ]\n",
      "in:[czy moge uzyc twojego olowka] -> out:[ may i use your a pencil ]\n",
      "in:[moja siostra jest piekna] -> out:[ my sister is beautiful ]\n"
     ]
    }
   ],
   "source": [
    "# create another translator\n",
    "translator.create_translator(\"lang/pol-eng\", \"trans/pol-eng\", epochs=8)\n",
    "\n",
    "# see how it works\n",
    "with open(\"lang/pol-eng/test_texts.txt\", \"rt\") as file_in:\n",
    "\tlines = file_in.readlines()\n",
    "\ttranslator.translate_sequences(\"lang/pol-eng\", \"trans/pol-eng\", random.sample(lines, 10))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#release all the gpu memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}