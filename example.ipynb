{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('what club do you belong to', '[start] do ktorego klubu nalezysz [end]')\n",
      "('its almost bedtime', '[start] juz prawie czas spac [end]')\n",
      "('its cloudy', '[start] niebo jest zachmurzone [end]')\n",
      "('tom almost told mary that he loved her', '[start] tom prawie powiedzial mary ze ja kocha [end]')\n",
      "('i cant figure out how to operate this machine', '[start] nie umiem obslugiwac tej maszyny [end]')\n",
      "41666 total pairs\n",
      "29168 training pairs\n",
      "6249 validation pairs\n",
      "6249 test pairs\n"
     ]
    }
   ],
   "source": [
    "# assemble text data for english->polish translation\n",
    "translator.prepare_text_zip(\"in/pol-eng.zip\",\"pol.txt\",\"lang/eng-pol\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The above will clean the text input, split the data into training/verification/test sets, and setup and fit vectorizers for training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('dlaczego nie idziesz bo nie chce', '[start] why arent you going because i dont want to [end]')\n",
      "('nie posiadam odpowiedzi', '[start] i dont have an answer [end]')\n",
      "('pamietaj aby wyslac te listy', '[start] please remember to mail the letters [end]')\n",
      "('chodz tu', '[start] come here [end]')\n",
      "('obawiam sie ze nie stac mnie na zakup nowego samochodu', '[start] im afraid i cant afford to buy a new car [end]')\n",
      "41666 total pairs\n",
      "29168 training pairs\n",
      "6249 validation pairs\n",
      "6249 test pairs\n"
     ]
    }
   ],
   "source": [
    "# text data for reverse translation\n",
    "translator.prepare_text_zip(\"in/pol-eng.zip\",\"pol.txt\",\"lang/pol-eng\", flip_translation=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (256, 16)\n",
      "inputs[\"decoder_inputs\"].shape: (256, 16)\n",
      "targets.shape: (256, 16)\n",
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding (  (None, None, 256)   12804096    ['encoder_inputs[0][0]']         \n",
      " TokenAndPositionEmbedding)                                                                       \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " TransformerEncoder (Transforme  (None, None, 256)   785652      ['token_and_position_embedding[0]\n",
      " rEncoder)                                                       [0]']                            \n",
      "                                                                                                  \n",
      " model_1 (Functional)           (None, None, 50000)  29336656    ['decoder_inputs[0][0]',         \n",
      "                                                                  'TransformerEncoder[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,926,404\n",
      "Trainable params: 42,926,404\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/2\n",
      "114/114 [==============================] - 25s 196ms/step - loss: 3.1308 - accuracy: 0.6514 - val_loss: 2.4495 - val_accuracy: 0.6979\n",
      "Epoch 2/2\n",
      "114/114 [==============================] - 22s 192ms/step - loss: 2.3881 - accuracy: 0.6938 - val_loss: 2.3488 - val_accuracy: 0.7049\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, position_embedding_layer_call_fn, position_embedding_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 916 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f01502c5750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:5 out of the last 916 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f01502c5750> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "114/114 [==============================] - 24s 196ms/step - loss: 2.2862 - accuracy: 0.7039 - val_loss: 2.2970 - val_accuracy: 0.7104\n",
      "Epoch 2/14\n",
      "114/114 [==============================] - 23s 197ms/step - loss: 2.1068 - accuracy: 0.7168 - val_loss: 2.2289 - val_accuracy: 0.7166\n",
      "Epoch 3/14\n",
      "114/114 [==============================] - 23s 201ms/step - loss: 1.9774 - accuracy: 0.7282 - val_loss: 2.0280 - val_accuracy: 0.7338\n",
      "Epoch 4/14\n",
      "114/114 [==============================] - 23s 202ms/step - loss: 1.8749 - accuracy: 0.7362 - val_loss: 1.9870 - val_accuracy: 0.7370\n",
      "Epoch 5/14\n",
      "114/114 [==============================] - 23s 203ms/step - loss: 1.7783 - accuracy: 0.7444 - val_loss: 1.9370 - val_accuracy: 0.7472\n",
      "Epoch 6/14\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 1.6838 - accuracy: 0.7531 - val_loss: 1.9094 - val_accuracy: 0.7511\n",
      "Epoch 7/14\n",
      "114/114 [==============================] - 23s 205ms/step - loss: 1.5974 - accuracy: 0.7608 - val_loss: 1.7978 - val_accuracy: 0.7595\n",
      "Epoch 8/14\n",
      "114/114 [==============================] - 24s 208ms/step - loss: 1.5062 - accuracy: 0.7697 - val_loss: 1.8103 - val_accuracy: 0.7544\n",
      "Epoch 9/14\n",
      "114/114 [==============================] - 23s 206ms/step - loss: 1.4165 - accuracy: 0.7788 - val_loss: 1.7710 - val_accuracy: 0.7695\n",
      "Epoch 10/14\n",
      "114/114 [==============================] - 23s 205ms/step - loss: 1.3252 - accuracy: 0.7885 - val_loss: 1.6880 - val_accuracy: 0.7748\n",
      "Epoch 11/14\n",
      "114/114 [==============================] - 23s 203ms/step - loss: 1.2417 - accuracy: 0.7977 - val_loss: 1.6120 - val_accuracy: 0.7833\n",
      "Epoch 12/14\n",
      "114/114 [==============================] - 23s 205ms/step - loss: 1.1428 - accuracy: 0.8089 - val_loss: 1.5931 - val_accuracy: 0.7836\n",
      "Epoch 13/14\n",
      "114/114 [==============================] - 23s 201ms/step - loss: 1.0600 - accuracy: 0.8187 - val_loss: 1.5723 - val_accuracy: 0.7917\n",
      "Epoch 14/14\n",
      "114/114 [==============================] - 23s 203ms/step - loss: 0.9713 - accuracy: 0.8298 - val_loss: 1.5325 - val_accuracy: 0.7931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_layer_call_fn, embedding_layer_call_and_return_conditional_losses, restored_function_body, restored_function_body, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/eng-pol/transformer.ts/assets\n"
     ]
    }
   ],
   "source": [
    "# create and pre-train translation model\n",
    "translator.create_translator(\"lang/eng-pol\", \"trans/eng-pol\", epochs=2)\n",
    "\n",
    "# train the model some more\n",
    "translator.train_model(\"lang/eng-pol\", \"trans/eng-pol\", 14)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 917 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f015c5de290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:6 out of the last 917 calls to <function PreprocessingLayer.make_adapt_function.<locals>.adapt_step at 0x7f015c5de290> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[Hello world] -> out:[ wenus jest glebokie ]\n",
      "in:[Good morning] -> out:[ rano jedza ]\n"
     ]
    }
   ],
   "source": [
    "# translate some phrases\n",
    "for seq_in,seq_out in translator.translate_sequences(\"lang/eng-pol\", \"trans/eng-pol\", [\"Hello world\", \"Good morning\"]):\n",
    "\tprint(\"in:[%s] -> out:[%s]\" % (seq_in, seq_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[we arent enemies] -> out:[ nie jestesmy rozczarowany ]\n",
      "in:[its very cold in here] -> out:[ jest bardzo za duzo ]\n",
      "in:[tom likes singing] -> out:[ tom lubi glebokie ]\n",
      "in:[tom didnt understand marys point of view] -> out:[ tom nie wiedzial ze mary ma urodziny ]\n",
      "in:[we have a reservation for sixthirty] -> out:[ mamy glebokie glebokie truskawek ]\n",
      "in:[i think its time for me to return to boston] -> out:[ mysle ze to dawno mysle ze tylko tylko tylko tylko tylko tylko godziny ]\n",
      "in:[keep it to yourself] -> out:[ zrob to z siebie ]\n",
      "in:[i want to know the reason] -> out:[ chce wiedziec nie bylo powodu ]\n",
      "in:[dont scare the kids] -> out:[ nie gap sie do dzieci ]\n",
      "in:[i hope i didnt wake you] -> out:[ mam nadzieje ze nie myslalem ]\n"
     ]
    }
   ],
   "source": [
    "# translate some sequences from file\n",
    "with open(\"lang/eng-pol/test_texts.txt\", \"rt\") as file_in:\n",
    "\tlines = file_in.readlines()\n",
    "\tfor seq_in,seq_out in translator.translate_sequences(\"lang/eng-pol\", \"trans/eng-pol\", random.sample(lines, 10)):\n",
    "\t\tprint(\"in:[%s] -> out:[%s]\" % (seq_in, seq_out))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs[\"encoder_inputs\"].shape: (256, 16)\n",
      "inputs[\"decoder_inputs\"].shape: (256, 16)\n",
      "targets.shape: (256, 16)\n",
      "Model: \"transformer\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " encoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " token_and_position_embedding_2  (None, None, 256)   12804096    ['encoder_inputs[0][0]']         \n",
      "  (TokenAndPositionEmbedding)                                                                     \n",
      "                                                                                                  \n",
      " decoder_inputs (InputLayer)    [(None, None)]       0           []                               \n",
      "                                                                                                  \n",
      " TransformerEncoder (Transforme  (None, None, 256)   785652      ['token_and_position_embedding_2[\n",
      " rEncoder)                                                       0][0]']                          \n",
      "                                                                                                  \n",
      " model_3 (Functional)           (None, None, 50000)  29336656    ['decoder_inputs[0][0]',         \n",
      "                                                                  'TransformerEncoder[0][0]']     \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 42,926,404\n",
      "Trainable params: 42,926,404\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/16\n",
      "114/114 [==============================] - 24s 199ms/step - loss: 3.1796 - accuracy: 0.5941 - val_loss: 3.8053 - val_accuracy: 0.3870\n",
      "Epoch 2/16\n",
      "114/114 [==============================] - 23s 198ms/step - loss: 2.4339 - accuracy: 0.6291 - val_loss: 2.3171 - val_accuracy: 0.6440\n",
      "Epoch 3/16\n",
      "114/114 [==============================] - 26s 229ms/step - loss: 2.2102 - accuracy: 0.6498 - val_loss: 2.0464 - val_accuracy: 0.6736\n",
      "Epoch 4/16\n",
      "114/114 [==============================] - 25s 219ms/step - loss: 1.9137 - accuracy: 0.6835 - val_loss: 1.8514 - val_accuracy: 0.6987\n",
      "Epoch 5/16\n",
      "114/114 [==============================] - 25s 221ms/step - loss: 1.6880 - accuracy: 0.7133 - val_loss: 1.6578 - val_accuracy: 0.7273\n",
      "Epoch 6/16\n",
      "114/114 [==============================] - 25s 217ms/step - loss: 1.4652 - accuracy: 0.7454 - val_loss: 1.4960 - val_accuracy: 0.7525\n",
      "Epoch 7/16\n",
      "114/114 [==============================] - 23s 206ms/step - loss: 1.2609 - accuracy: 0.7745 - val_loss: 1.4162 - val_accuracy: 0.7674\n",
      "Epoch 8/16\n",
      "114/114 [==============================] - 23s 206ms/step - loss: 1.0814 - accuracy: 0.8006 - val_loss: 1.2931 - val_accuracy: 0.7880\n",
      "Epoch 9/16\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 0.9276 - accuracy: 0.8234 - val_loss: 1.1922 - val_accuracy: 0.8007\n",
      "Epoch 10/16\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 0.7948 - accuracy: 0.8441 - val_loss: 1.1621 - val_accuracy: 0.8090\n",
      "Epoch 11/16\n",
      "114/114 [==============================] - 23s 203ms/step - loss: 0.6811 - accuracy: 0.8621 - val_loss: 1.1131 - val_accuracy: 0.8147\n",
      "Epoch 12/16\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 0.5767 - accuracy: 0.8802 - val_loss: 1.1085 - val_accuracy: 0.8187\n",
      "Epoch 13/16\n",
      "114/114 [==============================] - 24s 208ms/step - loss: 0.4864 - accuracy: 0.8958 - val_loss: 1.1028 - val_accuracy: 0.8238\n",
      "Epoch 14/16\n",
      "114/114 [==============================] - 24s 209ms/step - loss: 0.4040 - accuracy: 0.9116 - val_loss: 1.1124 - val_accuracy: 0.8214\n",
      "Epoch 15/16\n",
      "114/114 [==============================] - 23s 205ms/step - loss: 0.3354 - accuracy: 0.9252 - val_loss: 1.1602 - val_accuracy: 0.8196\n",
      "Epoch 16/16\n",
      "114/114 [==============================] - 23s 204ms/step - loss: 0.2755 - accuracy: 0.9373 - val_loss: 1.1607 - val_accuracy: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as embedding_2_layer_call_fn, embedding_2_layer_call_and_return_conditional_losses, position_embedding_2_layer_call_fn, position_embedding_2_layer_call_and_return_conditional_losses, multi_head_attention_layer_call_fn while saving (showing 5 of 68). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/pol-eng/transformer.ts/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: trans/pol-eng/transformer.ts/assets\n"
     ]
    }
   ],
   "source": [
    "# create another translator\n",
    "translator.create_translator(\"lang/pol-eng\", \"trans/pol-eng\", epochs=16)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in:[czy w poblizu jest jakis bank] -> out:[ is there any any traffic air of the bank ]\n",
      "in:[chyba potrzebujesz pomocy] -> out:[ you guess i need help ]\n",
      "in:[zaczeli tanczyc] -> out:[ they started dance ]\n",
      "in:[mam nadzieje ze nikogo z nich nie dotyczyl ten wypadek] -> out:[ i hope no one of them didnt eat that day ]\n",
      "in:[telefon wciaz dzwonil] -> out:[ the telephone was still called ]\n",
      "in:[robie to od lat] -> out:[ im doing it from years ]\n",
      "in:[tom jest niesmialy] -> out:[ tom is shy ]\n",
      "in:[tom powinien przeprosic mary za to ze nie przyszedl na czas] -> out:[ tom should apologize to mary for that ]\n",
      "in:[zawsze pijesz kawe do sniadania] -> out:[ do you drink coffee to eat breakfast ]\n",
      "in:[pokoj od dluzszego czasu stal pusty] -> out:[ room has been said since eating for some empty ]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# see how it works\n",
    "with open(\"lang/pol-eng/test_texts.txt\", \"rt\") as file_in:\n",
    "\tlines = file_in.readlines()\n",
    "\tfor seq_in,seq_out in translator.translate_sequences(\"lang/pol-eng\", \"trans/pol-eng\", random.sample(lines, 10)):\n",
    "\t\tprint(\"in:[%s] -> out:[%s]\" % (seq_in, seq_out))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "Could not synchronize CUDA stream: CUDA_ERROR_INVALID_CONTEXT: invalid device context",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mInternalError\u001B[0m                             Traceback (most recent call last)",
      "Input \u001B[0;32mIn [11]\u001B[0m, in \u001B[0;36m<cell line: 12>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      8\u001B[0m \tlines \u001B[38;5;241m=\u001B[39m file_in\u001B[38;5;241m.\u001B[39mreadlines()\n\u001B[1;32m      9\u001B[0m \tsampled \u001B[38;5;241m=\u001B[39m random\u001B[38;5;241m.\u001B[39msample(lines, \u001B[38;5;241m25\u001B[39m)\n\u001B[0;32m---> 12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m seq_in,seq_out \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtranslator\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtranslate_sequences\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlang/pol-eng\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtrans/pol-eng\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msampled\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[1;32m     13\u001B[0m \ttranslations\u001B[38;5;241m.\u001B[39mappend(seq_out)\n\u001B[1;32m     15\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m seq_in,seq_out \u001B[38;5;129;01min\u001B[39;00m translator\u001B[38;5;241m.\u001B[39mtranslate_sequences(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlang/eng-pol\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrans/eng-pol\u001B[39m\u001B[38;5;124m\"\u001B[39m, translations):\n",
      "File \u001B[0;32m~/PycharmProjects/wtad_nlk/translator.py:390\u001B[0m, in \u001B[0;36mtranslate_sequences\u001B[0;34m(input_dir, translator_dir, sequences)\u001B[0m\n\u001B[1;32m    383\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mtranslate_sequences\u001B[39m(input_dir: \u001B[38;5;28mstr\u001B[39m, translator_dir: \u001B[38;5;28mstr\u001B[39m, sequences:[\u001B[38;5;28mstr\u001B[39m]):\n\u001B[1;32m    384\u001B[0m     \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    385\u001B[0m \u001B[38;5;124;03m    Uses a trained model to translate string sequence list passed via @sequences param\u001B[39;00m\n\u001B[1;32m    386\u001B[0m \u001B[38;5;124;03m    :param input_dir:\u001B[39;00m\n\u001B[1;32m    387\u001B[0m \u001B[38;5;124;03m    :param translator_dir:\u001B[39;00m\n\u001B[1;32m    388\u001B[0m \u001B[38;5;124;03m    :param sequences:\u001B[39;00m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 390\u001B[0m     pickled_data, source_vectorization, target_vectorization \u001B[38;5;241m=\u001B[39m \u001B[43mload_text_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43minput_dir\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    392\u001B[0m     target_vocab \u001B[38;5;241m=\u001B[39m target_vectorization\u001B[38;5;241m.\u001B[39mget_vocabulary()\n\u001B[1;32m    393\u001B[0m     target_index_lookup \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mdict\u001B[39m(\u001B[38;5;28mzip\u001B[39m(\u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(target_vocab)), target_vocab))\n",
      "File \u001B[0;32m~/PycharmProjects/wtad_nlk/translator.py:167\u001B[0m, in \u001B[0;36mload_text_data\u001B[0;34m(data_dir)\u001B[0m\n\u001B[1;32m    164\u001B[0m target_vectorization \u001B[38;5;241m=\u001B[39m TextVectorization\u001B[38;5;241m.\u001B[39mfrom_config(pickled_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mtarget_vectorization\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mconfig\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    166\u001B[0m \u001B[38;5;66;03m# You have to call `adapt` with some dummy data (BUG in Keras)\u001B[39;00m\n\u001B[0;32m--> 167\u001B[0m \u001B[43msource_vectorization\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_tensor_slices\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mxyz\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    168\u001B[0m source_vectorization\u001B[38;5;241m.\u001B[39mset_weights(pickled_data[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124msource_vectorization\u001B[39m\u001B[38;5;124m'\u001B[39m][\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mweights\u001B[39m\u001B[38;5;124m'\u001B[39m])\n\u001B[1;32m    170\u001B[0m target_vectorization\u001B[38;5;241m.\u001B[39madapt(tf\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mDataset\u001B[38;5;241m.\u001B[39mfrom_tensor_slices([\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mxyz\u001B[39m\u001B[38;5;124m\"\u001B[39m]))\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/keras/layers/preprocessing/text_vectorization.py:428\u001B[0m, in \u001B[0;36mTextVectorization.adapt\u001B[0;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[1;32m    381\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21madapt\u001B[39m(\u001B[38;5;28mself\u001B[39m, data, batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m, steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[1;32m    382\u001B[0m   \u001B[38;5;124;03m\"\"\"Computes a vocabulary of string terms from tokens in a dataset.\u001B[39;00m\n\u001B[1;32m    383\u001B[0m \n\u001B[1;32m    384\u001B[0m \u001B[38;5;124;03m  Calling `adapt()` on a `TextVectorization` layer is an alternative to\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    426\u001B[0m \u001B[38;5;124;03m        argument is not supported with array inputs.\u001B[39;00m\n\u001B[1;32m    427\u001B[0m \u001B[38;5;124;03m  \"\"\"\u001B[39;00m\n\u001B[0;32m--> 428\u001B[0m   \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43madapt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msteps\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/keras/engine/base_preprocessing_layer.py:238\u001B[0m, in \u001B[0;36mPreprocessingLayer.adapt\u001B[0;34m(self, data, batch_size, steps)\u001B[0m\n\u001B[1;32m    236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuilt:\n\u001B[1;32m    237\u001B[0m   \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreset_state()\n\u001B[0;32m--> 238\u001B[0m data_handler \u001B[38;5;241m=\u001B[39m \u001B[43mdata_adapter\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataHandler\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    239\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    240\u001B[0m \u001B[43m    \u001B[49m\u001B[43mbatch_size\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mbatch_size\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    241\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_epoch\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msteps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    242\u001B[0m \u001B[43m    \u001B[49m\u001B[43mepochs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m    243\u001B[0m \u001B[43m    \u001B[49m\u001B[43msteps_per_execution\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    244\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdistribute\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m    245\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_adapt_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmake_adapt_function()\n\u001B[1;32m    246\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m _, iterator \u001B[38;5;129;01min\u001B[39;00m data_handler\u001B[38;5;241m.\u001B[39menumerate_epochs():\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/keras/engine/data_adapter.py:1168\u001B[0m, in \u001B[0;36mDataHandler.__init__\u001B[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001B[0m\n\u001B[1;32m   1165\u001B[0m strategy \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mdistribute\u001B[38;5;241m.\u001B[39mget_strategy()\n\u001B[1;32m   1167\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_current_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m\n\u001B[0;32m-> 1168\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_step_increment \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_steps_per_execution\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mitem() \u001B[38;5;241m-\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[1;32m   1169\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_insufficient_data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m   1171\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_configure_dataset_and_inferred_steps(strategy, x, steps_per_epoch,\n\u001B[1;32m   1172\u001B[0m                                            class_weight, distribute)\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/tensorflow/python/ops/resource_variable_ops.py:637\u001B[0m, in \u001B[0;36mBaseResourceVariable.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    635\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mnumpy\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[1;32m    636\u001B[0m   \u001B[38;5;28;01mif\u001B[39;00m context\u001B[38;5;241m.\u001B[39mexecuting_eagerly():\n\u001B[0;32m--> 637\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mread_value\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnumpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    638\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m(\n\u001B[1;32m    639\u001B[0m       \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnumpy() is only available when eager execution is enabled.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1159\u001B[0m, in \u001B[0;36m_EagerTensorBase.numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1136\u001B[0m \u001B[38;5;124;03m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001B[39;00m\n\u001B[1;32m   1137\u001B[0m \n\u001B[1;32m   1138\u001B[0m \u001B[38;5;124;03mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m   1156\u001B[0m \u001B[38;5;124;03m    NumPy dtype.\u001B[39;00m\n\u001B[1;32m   1157\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m   1158\u001B[0m \u001B[38;5;66;03m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001B[39;00m\n\u001B[0;32m-> 1159\u001B[0m maybe_arr \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_numpy\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[1;32m   1160\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m maybe_arr\u001B[38;5;241m.\u001B[39mcopy() \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(maybe_arr, np\u001B[38;5;241m.\u001B[39mndarray) \u001B[38;5;28;01melse\u001B[39;00m maybe_arr\n",
      "File \u001B[0;32m/usr/lib/python3.10/site-packages/tensorflow/python/framework/ops.py:1127\u001B[0m, in \u001B[0;36m_EagerTensorBase._numpy\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1125\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_numpy_internal()\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_NotOkStatusException \u001B[38;5;28;01mas\u001B[39;00m e:  \u001B[38;5;66;03m# pylint: disable=protected-access\u001B[39;00m\n\u001B[0;32m-> 1127\u001B[0m   \u001B[38;5;28;01mraise\u001B[39;00m core\u001B[38;5;241m.\u001B[39m_status_to_exception(e) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
      "\u001B[0;31mInternalError\u001B[0m: Could not synchronize CUDA stream: CUDA_ERROR_INVALID_CONTEXT: invalid device context"
     ]
    }
   ],
   "source": [
    "# compare in/out\n",
    "\n",
    "sampled = []\n",
    "translations = []\n",
    "roundtrip = []\n",
    "\n",
    "with open(\"lang/pol-eng/test_texts.txt\", \"rt\") as file_in:\n",
    "\tlines = file_in.readlines()\n",
    "\tsampled = random.sample(lines, 25)\n",
    "\n",
    "\n",
    "for seq_in,seq_out in translator.translate_sequences(\"lang/pol-eng\", \"trans/pol-eng\", sampled):\n",
    "\ttranslations.append(seq_out)\n",
    "\n",
    "for seq_in,seq_out in translator.translate_sequences(\"lang/eng-pol\", \"trans/eng-pol\", translations):\n",
    "\troundtrip.append(seq_out)\n",
    "\n",
    "from difflib import SequenceMatcher\n",
    "for i in range(len(sampled)):\n",
    "\tratio = SequenceMatcher(None, sampled[i], roundtrip[i]).ratio()\n",
    "\tprint(\"in:[%s] \\n\\t -> translated:[%s] \\n\\t -> roundtrip:[%s] \\n\\t accuracy=%f\" % (sampled[i].replace(\"\\n\",\"\"), translations[i], roundtrip[i], ratio))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "#release all the gpu memory\n",
    "from numba import cuda\n",
    "cuda.select_device(0)\n",
    "cuda.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}